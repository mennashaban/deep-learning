{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7217676,"sourceType":"datasetVersion","datasetId":4177201}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_dir = \"/kaggle/input/model-2/dataset/train\"\ntest_dir = \"/kaggle/input/model-2/dataset/test\"\n\n# Set the target image size\nimg_width, img_height = 224, 224\n\n# Set the batch size\nbatch_size = 32\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2  # 20% of the data will be used for validation\n)\n\n# Create the training data generator\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'  # Specify that this is the training subset\n)\n\n# Create the validation data generator\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'  # Specify that this is the validation subset\n)\n\nmodel = Sequential()\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(img_width, img_height, 3)))\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D(64,(2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(4096, activation='relu'))  # 3000 = No of neurons\nmodel.add(Dropout(0.1))  # regularization\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(train_generator.num_classes, activation='softmax'))\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nepochs =20\nmodel.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n\n# Calculate accuracy on the validation set\nval_loss, val_accuracy = model.evaluate(validation_generator)\nprint(\"Validation accuracy:\", val_accuracy)\n\ntest_files = [f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))]\n\ntest_images = []\nfor filename in test_files:\n    img_path = os.path.join(test_dir, filename)\n    img = load_img(img_path, target_size=(img_width, img_height))\n    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n    test_images.append(img_array)\n\n# Convert the list of images to a numpy array\ntest_images = np.array(test_images)\n\n# Predict the classes for the test images\npredictions = model.predict(test_images)\n\n# Convert predictions to class labels\npredicted_labels = np.argmax(predictions, axis=1) + 1\n\n# Print the predicted labels\nprint(\"Predicted labels:\", predicted_labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T23:13:40.783822Z","iopub.execute_input":"2023-12-23T23:13:40.78419Z","iopub.status.idle":"2023-12-23T23:18:09.744251Z","shell.execute_reply.started":"2023-12-23T23:13:40.784161Z","shell.execute_reply":"2023-12-23T23:18:09.742419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_dir = \"/kaggle/input/model-2/dataset/train\"\ntest_dir = \"/kaggle/input/model-2/dataset/test\"\n\n# Set the target image size\nimg_width, img_height = 299, 299\n\n# Set the batch size\nbatch_size = 32\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2  # 20% of the data will be used for validation\n)\n\n# Create the training data generator\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'  # Specify that this is the training subset\n)\n\n# Create the validation data generator\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'  # Specify that this is the validation subset\n)\n\nmodel = Sequential()\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(img_width, img_height, 3)))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(3000, activation='relu'))  # 3000 = No of neurons\nmodel.add(Dropout(0.1))  # regularization\nmodel.add(Dense(3000, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(train_generator.num_classes, activation='softmax'))\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nepochs =20\n\nmodel.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n\n# Calculate accuracy on the validation set\nval_loss, val_accuracy = model.evaluate(validation_generator)\nprint(\"Validation accuracy:\", val_accuracy)\n\ntest_files = [f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))]\n\ntest_images = []\nfor filename in test_files:\n    img_path = os.path.join(test_dir, filename)\n    img = load_img(img_path, target_size=(img_width, img_height))\n    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n    test_images.append(img_array)\n\n# Convert the list of images to a numpy array\ntest_images = np.array(test_images)\n\n# Predict the classes for the test images\npredictions = model.predict(test_images)\n\n# Convert predictions to class labels\npredicted_labels = np.argmax(predictions, axis=1) + 1\n\n# Print the predicted labels\nprint(\"Predicted labels:\", predicted_labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T23:20:46.78904Z","iopub.execute_input":"2023-12-23T23:20:46.789472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install einops\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T20:29:01.358014Z","iopub.execute_input":"2023-12-23T20:29:01.358371Z","iopub.status.idle":"2023-12-23T20:29:14.192768Z","shell.execute_reply.started":"2023-12-23T20:29:01.358341Z","shell.execute_reply":"2023-12-23T20:29:14.191679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models import resnet18\nfrom torch.nn import functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import random_split\nfrom torchvision import transforms\nfrom einops import rearrange\nimport math \nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport sys\n\n# Set the target image size\nimg_width, img_height = 299, 299\n\n# Set the batch size\nbatch_size = 50\n\n# Set the paths for the train and test directories\ntrain_dir = \"/kaggle/input/model-2/dataset/train\"\ntest_dir = \"/kaggle/input/model-2/dataset/test\"\n\n# Set up image data generators with data augmentation for the training set\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop((img_height, img_width)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n])\n\n# Create the training data loader with the defined transformations\ntrain_dataset = ImageFolder(train_dir, transform=train_transform)\n\n# Split the dataset into training and validation sets\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n# Create data loaders for training and validation sets\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n        super(PatchEmbedding, self).__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.embed_dim = embed_dim\n\n    def forward(self, x):\n        x = rearrange(x, 'b c h w -> b (h w c)')\n        #print(\"Shape after rearrangement:\", x.shape)\n        \n        # Initialize the linear layer based on the shape of x\n        self.projection = nn.Linear(x.shape[1], self.embed_dim)\n        x = self.projection(x)\n        return x\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, embed_dim, max_seq_len=512):\n        super(PositionalEncoding, self).__init__()\n        self.embed_dim = embed_dim\n        self.register_buffer('pe', self._get_positional_encoding(max_seq_len, embed_dim))\n\n    def _get_positional_encoding(self, max_seq_len, embed_dim):\n        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n        pe = torch.zeros(max_seq_len, embed_dim)\n        pe[:, 0::2] = torch.sin(position * div_term) #even positions\n        pe[:, 1::2] = torch.cos(position * div_term)  #odd positions\n        pe = pe.unsqueeze(0)\n        return pe\n\n    def forward(self, x):\n        # Check if x is 3D (batch_size, seq_len, embed_dim)\n        if x.dim() == 3:\n            return x + self.pe[:, :x.size(1)].detach()\n        else:\n            raise ValueError(f\"Unsupported input shape: {x.shape}\")\n\n\n\n\n\n# Transformer Encoder block\n\nclass TransformerEncoderBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_dim):\n        super(TransformerEncoderBlock, self).__init__()\n        self.embed_dim = embed_dim\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads) #num_head==> more than head (hyperparam)\n        self.mlp = MLPBlock(embed_dim, mlp_dim)\n\n    def forward(self, x):\n        # Self-attention\n        residual = x\n        x = self.ln1(x)\n        x = self.attention(x, x, x)[0]\n        x = x + residual\n\n        # MLP\n        residual = x\n        x = self.ln2(x)\n        x = self.mlp(x)\n        x = x + residual\n\n        return x\n    \n    \n# MLP block\nclass MLPBlock(nn.Sequential):  # training here of features\n    def __init__(self, embed_dim, mlp_dim):\n        super(MLPBlock, self).__init__(\n            nn.Linear(embed_dim, mlp_dim), #layer\n            nn.GELU(), #act fn \n            nn.Dropout(0.1), #no overfitting\n            nn.Linear(mlp_dim, embed_dim), #layer\n            nn.Dropout(0.1)\n        )\n        \n        \nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, num_heads, mlp_dim, num_layers):\n        super(VisionTransformer, self).__init__()\n        \n        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        self.positional_encoding = PositionalEncoding(embed_dim)\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim)\n            for _ in range(num_layers)\n        ])\n\n        self.ln = nn.LayerNorm(embed_dim)\n        self.fc = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        x = self.patch_embedding(x)\n        x = x.view(x.size(0), -1, self.patch_embedding.embed_dim)  # Reshape to (batch_size, seq_len, embed_dim)\n        x = self.positional_encoding(x)\n        \n        for block in self.transformer_blocks:\n            x = block(x)\n\n        x = self.ln(x.mean(dim=1))  # Global average pooling\n        x = self.fc(x)\n\n        return x\n\n# Initialize the model\nmodel = VisionTransformer(\n    img_size=(img_width, img_height),\n    patch_size=16,\n    in_channels=3,\n    num_classes=5,\n    embed_dim=256, #size of vector that out from embbeding #before = 256\n    num_heads=8, #before = 8 \n    mlp_dim=512,  #before=512\n    num_layers=6 #before=6\n)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters()) #upadte weights\n\nimport sys\n\nepochs =1\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    # Print the length of the training generator\n    print(f'Training generator length: {len(train_loader)}')\n\n    # Use the data generated by train_generator\n    for i, (inputs, labels) in enumerate(train_loader):\n        # Convert inputs and labels to PyTorch tensors\n        inputs, labels = torch.tensor(inputs), torch.tensor(labels)\n\n        # Ensure the inputs are in the correct format (NCHW)\n        inputs = inputs.permute(0, 3, 1, 2).contiguous()\n\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        # Calculate training accuracy\n        _, predicted = torch.max(outputs, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels.argmax(dim=0)).sum().item()\n\n        # Print loss for each iteration\n        print(f'Epoch {epoch + 1}/{epochs}, Iteration {i + 1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n\n        # Break the loop if you want to iterate only up to len(train_generator) batches\n        if i + 1 == len(train_loader):\n            break\n\n    # Calculate training accuracy\n    train_accuracy = (correct_train / total_train) * 100\n        # Validation phase\n    model.eval()\n    correct_val = 0\n    total_val = 0\n\n    with torch.no_grad():\n        for inputs_val, labels_val in val_loader:\n            # Convert inputs and labels to PyTorch tensors\n            inputs_val, labels_val = torch.tensor(inputs_val), torch.tensor(labels_val)\n\n            # Ensure the inputs are in the correct format (NCHW)\n            inputs_val = inputs_val.permute(0, 3, 1, 2).contiguous()\n\n            # Make predictions\n            outputs_val = model(inputs_val)\n            _, predicted_val = torch.max(outputs_val, 1)\n            total_val += labels_val.size(0)\n            correct_val += (predicted_val == labels_val.argmax(dim=0)).sum().item()\n\n    # Calculate validation accuracy\n    val_accuracy = (correct_val / total_val) * 100\n\n    # Print epoch information with validation accuracy\n    print(f'Epoch {epoch + 1}/{epochs}, '\n          f'Training Loss: {running_loss / len(train_loader):.4f}, '\n          f'Training Accuracy: {train_accuracy:.2f}%, '\n          f'Validation Accuracy: {val_accuracy:.2f}%')\n    sys.stdout.flush()  # Flush the output but\n# Prepare the test data\ntest_files = [f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))]\n\ntest_images = []\nfor filename in test_files:\n    img_path = os.path.join(test_dir, filename)\n    img = load_img(img_path, target_size=(img_width, img_height))\n    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n    test_images.append(img_array)\n\n# Convert the list of images to a numpy array\ntest_images = np.array(test_images)\n\n# Convert numpy array to PyTorch tensor\ntest_images = torch.tensor(np.transpose(test_images, (0, 3, 1, 2)), dtype=torch.float32)\n\n\n# Make predictions on the test set\nwith torch.no_grad():\n    model.eval()\n\n    predictions = model(test_images)\n\n# Convert predictions to class labels\npredicted_labels = torch.argmax(predictions, axis=1).numpy() + 1\n\n# Print the predicted labels\nprint(\"Predicted labels:\", predicted_labels)\n\n# (Insert code for testing the model on the test dataset here)    ","metadata":{"execution":{"iopub.status.busy":"2023-12-23T22:49:02.451098Z","iopub.execute_input":"2023-12-23T22:49:02.452221Z","iopub.status.idle":"2023-12-23T22:52:38.763883Z","shell.execute_reply.started":"2023-12-23T22:49:02.452179Z","shell.execute_reply":"2023-12-23T22:52:38.762811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nresult_df = pd.DataFrame({\n    'image_id': [filename.split('.')[0] for filename in test_files],  # Extracting image names without extension\n    'label': predicted_labels\n})\n\n# Save the DataFrame to a CSV file\nresult_csv_path = \"/kaggle/working/comp5.csv\"\nresult_df.to_csv(result_csv_path, index=False)\n\n# Print the DataFrame\nprint(result_df)\n\n# Print a message indicating the CSV file path\nprint(f\"Predictions saved to: {result_csv_path}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T21:47:25.430113Z","iopub.execute_input":"2023-12-22T21:47:25.430535Z","iopub.status.idle":"2023-12-22T21:47:25.447608Z","shell.execute_reply.started":"2023-12-22T21:47:25.430485Z","shell.execute_reply":"2023-12-22T21:47:25.446436Z"},"trusted":true},"execution_count":null,"outputs":[]}]}